# ğŸš¨ ç½å®³å¾©æ—§ï¼ˆDRï¼‰ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

k8s_myHomeã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ç½å®³å¾©æ—§è¨ˆç”»ï¼ˆDRP: Disaster Recovery Planï¼‰ã‚’å®šç¾©ã—ã€å„ç¨®éšœå®³ã‚·ãƒŠãƒªã‚ªã«å¯¾ã™ã‚‹å¾©æ—§æ‰‹é †ã‚’æä¾›ã—ã¾ã™ã€‚

## DRæˆ¦ç•¥

### å¾©æ—§ç›®æ¨™

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ç›®æ¨™å€¤ | èª¬æ˜ |
|-----------|--------|------|
| **RTO** (Recovery Time Objective) | 4æ™‚é–“ | ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§ã¾ã§ã®ç›®æ¨™æ™‚é–“ |
| **RPO** (Recovery Point Objective) | 24æ™‚é–“ | è¨±å®¹å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿æå¤±æœŸé–“ |
| **MTTR** (Mean Time To Recovery) | 2æ™‚é–“ | å¹³å‡å¾©æ—§æ™‚é–“ |
| **å¯ç”¨æ€§ç›®æ¨™** | 99.5% | å¹´é–“ç¨¼åƒç‡ï¼ˆç´„43æ™‚é–“ã®ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ è¨±å®¹ï¼‰ |

### éšœå®³ãƒ¬ãƒ™ãƒ«åˆ†é¡

```mermaid
graph TD
    Failure[éšœå®³ç™ºç”Ÿ] --> Level{éšœå®³ãƒ¬ãƒ™ãƒ«}
    Level -->|Level 1| L1[ã‚µãƒ¼ãƒ“ã‚¹éƒ¨åˆ†åœæ­¢<br/>å½±éŸ¿: ä½]
    Level -->|Level 2| L2[ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢<br/>å½±éŸ¿: ä¸­]
    Level -->|Level 3| L3[ã‚·ã‚¹ãƒ†ãƒ éšœå®³<br/>å½±éŸ¿: é«˜]
    Level -->|Level 4| L4[å®Œå…¨éšœå®³<br/>å½±éŸ¿: è‡´å‘½çš„]
    
    L1 --> R1[è‡ªå‹•å¾©æ—§<br/>15åˆ†ä»¥å†…]
    L2 --> R2[æ‰‹å‹•ä»‹å…¥<br/>1æ™‚é–“ä»¥å†…]
    L3 --> R3[éƒ¨åˆ†å†æ§‹ç¯‰<br/>2æ™‚é–“ä»¥å†…]
    L4 --> R4[å®Œå…¨å†æ§‹ç¯‰<br/>4æ™‚é–“ä»¥å†…]
```

## éšœå®³ã‚·ãƒŠãƒªã‚ªã¨å¯¾å¿œ

### Level 1: ã‚µãƒ¼ãƒ“ã‚¹éƒ¨åˆ†åœæ­¢

#### ã‚·ãƒŠãƒªã‚ª: Podç•°å¸¸çµ‚äº†
```bash
# æ¤œçŸ¥
kubectl get pods --all-namespaces | grep -v Running

# è‡ªå‹•å¾©æ—§ï¼ˆReplicaSet/Deploymentã«ã‚ˆã‚‹ï¼‰
# æ‰‹å‹•ä»‹å…¥ãŒå¿…è¦ãªå ´åˆ
kubectl rollout restart deployment/<name> -n <namespace>

# ç¢ºèª
kubectl get events -n <namespace> --sort-by='.lastTimestamp'
```

#### ã‚·ãƒŠãƒªã‚ª: ä¸€æ™‚çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³
```bash
# æ¤œçŸ¥
kubectl get endpoints --all-namespaces

# å¾©æ—§
kubectl rollout restart daemonset/kube-proxy -n kube-system
kubectl rollout restart deployment/coredns -n kube-system

# ç¢ºèª
for pod in $(kubectl get pods -n kube-system -o name); do
  kubectl exec -n kube-system $pod -- nslookup kubernetes.default
done
```

### Level 2: ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢

#### ã‚·ãƒŠãƒªã‚ª: ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰éšœå®³

```bash
#!/bin/bash
# worker-node-recovery.sh

FAILED_NODE=$1

# 1. ãƒãƒ¼ãƒ‰çŠ¶æ…‹ç¢ºèª
kubectl get node ${FAILED_NODE}
kubectl describe node ${FAILED_NODE}

# 2. Podã‚’ä»–ãƒãƒ¼ãƒ‰ã¸é€€é¿
kubectl drain ${FAILED_NODE} --ignore-daemonsets --delete-emptydir-data

# 3. ãƒãƒ¼ãƒ‰å¾©æ—§è©¦è¡Œ
ssh k8suser@${FAILED_NODE} "
  sudo systemctl restart kubelet
  sudo systemctl restart containerd
"

# 4. å¾©æ—§ç¢ºèªï¼ˆ5åˆ†å¾…æ©Ÿï¼‰
sleep 300
if kubectl get node ${FAILED_NODE} | grep -q "Ready"; then
  echo "ãƒãƒ¼ãƒ‰å¾©æ—§æˆåŠŸ"
  kubectl uncordon ${FAILED_NODE}
else
  echo "ãƒãƒ¼ãƒ‰å¾©æ—§å¤±æ•— - VMå†ä½œæˆãŒå¿…è¦"
  # VMå†ä½œæˆæ‰‹é †ã¸
fi
```

#### ã‚·ãƒŠãƒªã‚ª: ArgoCDéšœå®³

```bash
# 1. ArgoCD Podç¢ºèª
kubectl get pods -n argocd

# 2. ArgoCDå®Œå…¨å†èµ·å‹•
kubectl delete pods -n argocd --all

# 3. å¾©æ—§å¾…æ©Ÿã¨ç¢ºèª
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s

# 4. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åŒæœŸçŠ¶æ…‹ç¢ºèª
kubectl get applications -n argocd

# 5. æ‰‹å‹•åŒæœŸï¼ˆå¿…è¦ãªå ´åˆï¼‰
for app in $(kubectl get applications -n argocd -o jsonpath='{.items[*].metadata.name}'); do
  kubectl patch application $app -n argocd --type merge -p '{"operation":{"sync":{}}}'
done
```

### Level 3: ã‚·ã‚¹ãƒ†ãƒ éšœå®³

#### ã‚·ãƒŠãƒªã‚ª: etcdéšœå®³

```bash
#!/bin/bash
# etcd-disaster-recovery.sh

# 1. etcdçŠ¶æ…‹ç¢ºèª
kubectl exec -n kube-system etcd-k8s-control-plane-1 -- \
  etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list

# 2. å¥å…¨ãªãƒ¡ãƒ³ãƒãƒ¼ã‹ã‚‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å–å¾—
HEALTHY_MEMBER="etcd-k8s-control-plane-1"
kubectl exec -n kube-system ${HEALTHY_MEMBER} -- \
  etcdctl [...] snapshot save /tmp/emergency-backup.db

# 3. etcdã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åœæ­¢
sudo systemctl stop etcd

# 4. ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¯ãƒªã‚¢
sudo rm -rf /var/lib/etcd/member

# 5. ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰ãƒªã‚¹ãƒˆã‚¢
sudo ETCDCTL_API=3 etcdctl snapshot restore /tmp/emergency-backup.db \
  --name k8s-control-plane-1 \
  --initial-cluster k8s-control-plane-1=https://192.168.122.10:2380 \
  --initial-advertise-peer-urls https://192.168.122.10:2380 \
  --data-dir /var/lib/etcd

# 6. etcdå†èµ·å‹•
sudo chown -R etcd:etcd /var/lib/etcd
sudo systemctl start etcd

# 7. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç¢ºèª
kubectl get cs
kubectl get nodes
```

#### ã‚·ãƒŠãƒªã‚ª: ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³å®Œå…¨éšœå®³

```bash
#!/bin/bash
# control-plane-rebuild.sh

# 1. æ–°VMä½œæˆ
cd automation/infrastructure
terraform destroy -target=libvirt_domain.k8s-control-plane
terraform apply -target=libvirt_domain.k8s-control-plane

# 2. æ–°ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³åˆæœŸåŒ–
ssh k8suser@192.168.122.10 << 'EOF'
sudo kubeadm init \
  --apiserver-advertise-address=192.168.122.10 \
  --pod-network-cidr=10.244.0.0/16 \
  --service-cidr=10.96.0.0/12 \
  --upload-certs
EOF

# 3. etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ãƒªã‚¹ãƒˆã‚¢
scp /backup/latest/etcd-snapshot.db k8suser@192.168.122.10:/tmp/
ssh k8suser@192.168.122.10 << 'EOF'
sudo systemctl stop etcd
sudo rm -rf /var/lib/etcd/member
sudo ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-snapshot.db \
  --data-dir /var/lib/etcd
sudo chown -R etcd:etcd /var/lib/etcd
sudo systemctl start etcd
EOF

# 4. ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰å†å‚åŠ 
for worker in 192.168.122.11 192.168.122.12; do
  ssh k8suser@${worker} "sudo kubeadm reset -f"
  ssh k8suser@${worker} "sudo kubeadm join 192.168.122.10:6443 --token <token> --discovery-token-ca-cert-hash <hash>"
done
```

### Level 4: å®Œå…¨éšœå®³

#### ã‚·ãƒŠãƒªã‚ª: ãƒ›ã‚¹ãƒˆãƒã‚·ãƒ³éšœå®³

```bash
#!/bin/bash
# complete-disaster-recovery.sh

# å‰æ: æ–°ã—ã„ãƒ›ã‚¹ãƒˆãƒã‚·ãƒ³ãŒæº–å‚™æ¸ˆã¿
# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ãŒå¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰åˆ©ç”¨å¯èƒ½

set -e

log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

# 1. æ–°ãƒ›ã‚¹ãƒˆæº–å‚™
prepare_new_host() {
    log "æ–°ãƒ›ã‚¹ãƒˆæº–å‚™é–‹å§‹..."
    
    # OSæ›´æ–°
    sudo apt update && sudo apt upgrade -y
    
    # å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    cd k8s_myHome/automation/host-setup
    ./setup-host.sh
    ./setup-libvirt-sudo.sh
    ./setup-storage.sh
    ./verify-setup.sh
    
    log "æ–°ãƒ›ã‚¹ãƒˆæº–å‚™å®Œäº†"
}

# 2. ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£å†æ§‹ç¯‰
rebuild_infrastructure() {
    log "ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£å†æ§‹ç¯‰é–‹å§‹..."
    
    cd ../infrastructure
    terraform init
    terraform apply -auto-approve
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–å¾…æ©Ÿ
    sleep 600
    
    log "ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£å†æ§‹ç¯‰å®Œäº†"
}

# 3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ãƒªã‚¹ãƒˆã‚¢
restore_from_backup() {
    log "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒªã‚¹ãƒˆã‚¢é–‹å§‹..."
    
    # æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å–å¾—
    LATEST_BACKUP=$(ls -t /backup/*.tar.gz | head -1)
    
    # ãƒªã‚¹ãƒˆã‚¢å®Ÿè¡Œ
    /usr/local/bin/k8s-restore.sh ${LATEST_BACKUP}
    
    log "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒªã‚¹ãƒˆã‚¢å®Œäº†"
}

# 4. ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚µãƒ¼ãƒ“ã‚¹å†ãƒ‡ãƒ—ãƒ­ã‚¤
redeploy_platform() {
    log "ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚µãƒ¼ãƒ“ã‚¹å†ãƒ‡ãƒ—ãƒ­ã‚¤é–‹å§‹..."
    
    cd ../platform
    ./platform-deploy.sh
    
    log "ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚µãƒ¼ãƒ“ã‚¹å†ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†"
}

# 5. æ¤œè¨¼
verify_recovery() {
    log "å¾©æ—§æ¤œè¨¼é–‹å§‹..."
    
    # ãƒãƒ¼ãƒ‰ç¢ºèª
    kubectl get nodes
    
    # ã‚·ã‚¹ãƒ†ãƒ Podç¢ºèª
    kubectl get pods -n kube-system
    
    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºèª
    kubectl get applications -n argocd
    
    # ã‚µãƒ¼ãƒ“ã‚¹ç–é€šç¢ºèª
    curl -I http://192.168.122.100
    
    log "å¾©æ—§æ¤œè¨¼å®Œäº†"
}

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
main() {
    log "å®Œå…¨ç½å®³å¾©æ—§é–‹å§‹"
    
    prepare_new_host
    rebuild_infrastructure
    restore_from_backup
    redeploy_platform
    verify_recovery
    
    log "å®Œå…¨ç½å®³å¾©æ—§å®Œäº†"
}

# å®Ÿè¡Œç¢ºèª
read -p "å®Œå…¨ç½å®³å¾©æ—§ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ [y/N]: " confirm
if [ "$confirm" = "y" ]; then
    main
else
    echo "ä¸­æ­¢ã—ã¾ã—ãŸ"
    exit 0
fi
```

## å¾©æ—§æ‰‹é †æ›¸

### åˆå‹•å¯¾å¿œãƒ•ãƒ­ãƒ¼

```mermaid
graph TD
    Alert[ã‚¢ãƒ©ãƒ¼ãƒˆ/éšœå®³æ¤œçŸ¥] --> Assess[å½±éŸ¿è©•ä¾¡]
    Assess --> Level{éšœå®³ãƒ¬ãƒ™ãƒ«åˆ¤å®š}
    
    Level -->|Level 1-2| QuickFix[å³æ™‚å¯¾å¿œ]
    Level -->|Level 3-4| Escalate[ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
    
    QuickFix --> Monitor[ç›£è¦–ç¶™ç¶š]
    Escalate --> Team[å¯¾å¿œãƒãƒ¼ãƒ æ‹›é›†]
    
    Team --> Analyze[è©³ç´°åˆ†æ]
    Analyze --> Plan[å¾©æ—§è¨ˆç”»ç­–å®š]
    Plan --> Execute[å¾©æ—§å®Ÿè¡Œ]
    Execute --> Verify[æ¤œè¨¼]
    Verify --> Document[è¨˜éŒ²ä½œæˆ]
```

### é€£çµ¡ä½“åˆ¶

```yaml
# éšœå®³ãƒ¬ãƒ™ãƒ«åˆ¥é€£çµ¡å…ˆ
escalation:
  level1:
    - æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«é€šçŸ¥
    - Slacké€šçŸ¥
  level2:
    - æ‹…å½“è€…é›»è©±é€£çµ¡
    - ãƒãƒ¼ãƒ Slackãƒãƒ£ãƒ³ãƒãƒ«
  level3:
    - ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é€£çµ¡
    - ç·Šæ€¥å¯¾å¿œãƒãƒ¼ãƒ æ‹›é›†
  level4:
    - çµŒå–¶å±¤å ±å‘Š
    - å…¨ç¤¾é€šçŸ¥
```

## äºˆé˜²æªç½®

### å®šæœŸè¨“ç·´

```bash
#!/bin/bash
# dr-drill.sh - æœˆæ¬¡DRè¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

# 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèª
echo "=== ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèª ==="
ls -la /backup/*.tar.gz | tail -5

# 2. ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ãƒªã‚¹ãƒˆã‚¢æ¼”ç¿’
echo "=== ãƒ†ã‚¹ãƒˆç’°å¢ƒãƒªã‚¹ãƒˆã‚¢ ==="
kubectl create namespace dr-test
kubectl apply -f /backup/test-resources.yaml -n dr-test

# 3. ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ãƒ†ã‚¹ãƒˆ
echo "=== ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ãƒ†ã‚¹ãƒˆ ==="
kubectl drain k8s-worker-1 --ignore-daemonsets
sleep 60
kubectl get pods --all-namespaces -o wide | grep k8s-worker-2
kubectl uncordon k8s-worker-1

# 4. çµæœè¨˜éŒ²
echo "=== è¨“ç·´çµæœ ==="
echo "å®Ÿæ–½æ—¥: $(date)" >> /var/log/dr-drill.log
echo "æ‰€è¦æ™‚é–“: XXåˆ†" >> /var/log/dr-drill.log
echo "å•é¡Œç‚¹: ãªã—/ã‚ã‚Šï¼ˆè©³ç´°ï¼‰" >> /var/log/dr-drill.log
```

### ç›£è¦–å¼·åŒ–

```yaml
# Prometheus AlertManagerè¨­å®š
global:
  resolve_timeout: 5m

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
  - match:
      severity: critical
    receiver: 'critical'
    continue: true
  - match:
      severity: warning
    receiver: 'warning'

receivers:
- name: 'default'
  slack_configs:
  - api_url: '$SLACK_WEBHOOK_URL'
    channel: '#alerts'

- name: 'critical'
  pagerduty_configs:
  - service_key: '$PAGERDUTY_KEY'
  slack_configs:
  - api_url: '$SLACK_WEBHOOK_URL'
    channel: '#critical-alerts'

- name: 'warning'
  email_configs:
  - to: 'team@example.com'
```

## å¾©æ—§å¾Œä½œæ¥­

### ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] å…¨ã‚µãƒ¼ãƒ“ã‚¹ç¨¼åƒç¢ºèª
- [ ] ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç¢ºèª
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å†è¨­å®š
- [ ] ç›£è¦–ã‚¢ãƒ©ãƒ¼ãƒˆç¢ºèª
- [ ] ãƒ­ã‚°åé›†ãƒ»åˆ†æ
- [ ] ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
- [ ] æ”¹å–„ç‚¹ã®ç‰¹å®š
- [ ] DRPã®æ›´æ–°

### ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```markdown
# ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆ

## æ¦‚è¦
- **ç™ºç”Ÿæ—¥æ™‚**: YYYY-MM-DD HH:MM
- **å¾©æ—§å®Œäº†æ™‚åˆ»**: YYYY-MM-DD HH:MM
- **å½±éŸ¿æ™‚é–“**: XXæ™‚é–“XXåˆ†
- **éšœå®³ãƒ¬ãƒ™ãƒ«**: Level X
- **å½±éŸ¿ç¯„å›²**: 

## ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
- HH:MM - éšœå®³æ¤œçŸ¥
- HH:MM - åˆå‹•å¯¾å¿œé–‹å§‹
- HH:MM - åŸå› ç‰¹å®š
- HH:MM - å¾©æ—§ä½œæ¥­é–‹å§‹
- HH:MM - ã‚µãƒ¼ãƒ“ã‚¹å¾©æ—§
- HH:MM - å®Œå…¨å¾©æ—§ç¢ºèª

## åŸå› åˆ†æ
### ç›´æ¥åŸå› 
### æ ¹æœ¬åŸå› 
### å¯„ä¸è¦å› 

## å¯¾å¿œå†…å®¹
### å³æ™‚å¯¾å¿œ
### æ’ä¹…å¯¾ç­–

## æ”¹å–„ææ¡ˆ
1. 
2. 
3. 

## å­¦ã‚“ã æ•™è¨“
```

## DRæˆç†Ÿåº¦è©•ä¾¡

### ç¾åœ¨ã®ãƒ¬ãƒ™ãƒ«

| è©•ä¾¡é …ç›® | ãƒ¬ãƒ™ãƒ« | æ”¹å–„ç‚¹ |
|---------|--------|--------|
| ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è‡ªå‹•åŒ– | â˜…â˜…â˜…â˜…â˜† | ã‚ªãƒ•ã‚µã‚¤ãƒˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¿½åŠ  |
| å¾©æ—§æ‰‹é †æ–‡æ›¸åŒ– | â˜…â˜…â˜…â˜…â˜… | - |
| ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ | â˜…â˜…â˜…â˜†â˜† | Prometheus/Grafanaå°å…¥ |
| è¨“ç·´å®Ÿæ–½ | â˜…â˜…â˜…â˜†â˜† | æœˆæ¬¡è¨“ç·´ã®å®šç€ |
| RTO/RPOé”æˆ | â˜…â˜…â˜…â˜…â˜† | RTOã•ã‚‰ãªã‚‹çŸ­ç¸® |

### æ”¹å–„ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

```mermaid
gantt
    title DRæ”¹å–„ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—
    dateFormat  YYYY-MM-DD
    section Phase 1
    ã‚ªãƒ•ã‚µã‚¤ãƒˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—    :a1, 2025-01-15, 30d
    Prometheus/Grafanaå°å…¥     :a2, 2025-02-01, 45d
    section Phase 2
    ãƒãƒ«ãƒã‚µã‚¤ãƒˆæ§‹æˆæ¤œè¨       :b1, 2025-03-01, 60d
    è‡ªå‹•ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼å®Ÿè£…   :b2, 2025-04-01, 90d
    section Phase 3
    DRè‡ªå‹•åŒ–å®Œå…¨å®Ÿè£…          :c1, 2025-06-01, 120d
```

---
*æœ€çµ‚æ›´æ–°: 2025-01-09*