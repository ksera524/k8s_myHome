# ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

k8s_myHomeã§ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å•é¡Œã¨ã€ãã®è§£æ±ºæ–¹æ³•ã‚’ä½“ç³»çš„ã«ã¾ã¨ã‚ãŸã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

## å•é¡Œè¨ºæ–­ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```mermaid
graph TD
    Start[å•é¡Œç™ºç”Ÿ] --> Type{å•é¡Œã®ç¨®é¡}
    Type -->|èµ·å‹•ã—ãªã„| Boot[èµ·å‹•å•é¡Œ]
    Type -->|æ¥ç¶šã§ããªã„| Network[ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œ]
    Type -->|ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³| Resource[ãƒªã‚½ãƒ¼ã‚¹å•é¡Œ]
    Type -->|ã‚¢ãƒ—ãƒªã‚¨ãƒ©ãƒ¼| App[ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å•é¡Œ]
    
    Boot --> VM{VMèµ·å‹•?}
    VM -->|No| CheckLibvirt[libvirtç¢ºèª]
    VM -->|Yes| CheckK8s[Kubernetesç¢ºèª]
    
    Network --> Ping{Pingé€šã‚‹?}
    Ping -->|No| CheckRoute[ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç¢ºèª]
    Ping -->|Yes| CheckService[ã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª]
    
    Resource --> CheckCPU[CPU/ãƒ¡ãƒ¢ãƒªç¢ºèª]
    CheckCPU --> Scale[ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¤œè¨]
    
    App --> CheckPod[PodçŠ¶æ…‹ç¢ºèª]
    CheckPod --> CheckLogs[ãƒ­ã‚°ç¢ºèª]
```

## ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

## 1. ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£å•é¡Œ

### 1.1 VMèµ·å‹•å•é¡Œ

#### ç—‡çŠ¶: VMãŒèµ·å‹•ã—ãªã„
```bash
# çŠ¶æ…‹ç¢ºèª
sudo virsh list --all
# æœŸå¾…: runningçŠ¶æ…‹ã§ãªã„
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. libvirtã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª
sudo systemctl status libvirtd
sudo systemctl restart libvirtd

# 2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¢ºèª
sudo virsh net-list --all
sudo virsh net-start default

# 3. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç¢ºèª
sudo virsh pool-list --all
df -h /var/lib/libvirt/images

# 4. VMå¼·åˆ¶èµ·å‹•
sudo virsh start k8s-control-plane-1

# 5. ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç¢ºèª
sudo journalctl -u libvirtd -n 100
sudo virsh domblklist k8s-control-plane-1
```

#### ç—‡çŠ¶: VMãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹
```bash
# ãƒ¡ãƒ¢ãƒªä¸è¶³ãƒã‚§ãƒƒã‚¯
free -h
sudo virsh dominfo k8s-control-plane-1
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. ãƒªã‚½ãƒ¼ã‚¹èª¿æ•´
sudo virsh setmem k8s-control-plane-1 6G --config
sudo virsh setvcpus k8s-control-plane-1 3 --config

# 2. VMå†ä½œæˆ
cd automation/infrastructure
terraform destroy -target=libvirt_domain.k8s-control-plane-1
terraform apply
```

### 1.2 ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å•é¡Œ

#### ç—‡çŠ¶: ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³
```bash
# ã‚¨ãƒ©ãƒ¼ä¾‹
Error: cannot create volume: Storage volume allocation error
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. ä½¿ç”¨çŠ¶æ³ç¢ºèª
df -h
du -sh /var/lib/libvirt/images/*

# 2. ä¸è¦ãªã‚¤ãƒ¡ãƒ¼ã‚¸å‰Šé™¤
sudo virsh vol-list default
sudo virsh vol-delete --pool default <volume-name>

# 3. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ—ãƒ¼ãƒ«æ‹¡å¼µ
sudo virsh pool-destroy default
sudo virsh pool-undefine default
# setup-storage.shç·¨é›†ã—ã¦å®¹é‡å¤‰æ›´
./automation/host-setup/setup-storage.sh
```

## 2. Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å•é¡Œ

### 2.1 ãƒãƒ¼ãƒ‰å•é¡Œ

#### ç—‡çŠ¶: Node Not Ready
```bash
kubectl get nodes
# NAME                  STATUS     ROLES
# k8s-worker-1          NotReady   <none>
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. ãƒãƒ¼ãƒ‰è©³ç´°ç¢ºèª
kubectl describe node k8s-worker-1

# 2. SSHæ¥ç¶šã—ã¦ç¢ºèª
ssh k8suser@192.168.122.11

# 3. kubeletçŠ¶æ…‹ç¢ºèª
sudo systemctl status kubelet
sudo journalctl -u kubelet -n 100

# 4. ã‚³ãƒ³ãƒ†ãƒŠãƒ©ãƒ³ã‚¿ã‚¤ãƒ ç¢ºèª
sudo systemctl status containerd
sudo crictl ps

# 5. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¢ºèª
ip a show flannel.1
sudo systemctl restart systemd-networkd

# 6. kubeletå†èµ·å‹•
sudo systemctl restart kubelet
```

#### ç—‡çŠ¶: etcdã‚¨ãƒ©ãƒ¼
```bash
# ãƒ­ã‚°ä¾‹
etcdserver: mvcc: database space exceeded
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. etcdã‚³ãƒ³ãƒ‘ã‚¯ã‚·ãƒ§ãƒ³
kubectl exec -n kube-system etcd-k8s-control-plane-1 -- \
  etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  compact $(etcdctl [...] endpoint status --write-out="json" | jq -r '.[] | .Status.header.revision')

# 2. ãƒ‡ãƒ•ãƒ©ã‚°
kubectl exec -n kube-system etcd-k8s-control-plane-1 -- \
  etcdctl [...] defrag

# 3. ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆ
kubectl exec -n kube-system etcd-k8s-control-plane-1 -- \
  etcdctl [...] snapshot save /var/lib/etcd/backup.db
```

### 2.2 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œ

#### ç—‡çŠ¶: Podé–“é€šä¿¡ä¸å¯
```bash
# ãƒ†ã‚¹ãƒˆ
kubectl run test-pod --image=busybox --rm -it -- sh
ping <other-pod-ip>
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. CNIç¢ºèª
kubectl get pods -n kube-flannel
kubectl logs -n kube-flannel -l app=flannel

# 2. iptablesç¢ºèª
sudo iptables -L -n -v | grep FORWARD
sudo iptables -t nat -L -n -v

# 3. Flannelå†ãƒ‡ãƒ—ãƒ­ã‚¤
kubectl delete -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```

#### ç—‡çŠ¶: Serviceæ¥ç¶šä¸å¯
```bash
kubectl get svc
# ClusterIPã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. Endpointsç¢ºèª
kubectl get endpoints <service-name>

# 2. kube-proxyç¢ºèª
kubectl get pods -n kube-system -l k8s-app=kube-proxy
kubectl logs -n kube-system -l k8s-app=kube-proxy

# 3. iptables rulesç¢ºèª
sudo iptables-save | grep <service-cluster-ip>

# 4. kube-proxyå†èµ·å‹•
kubectl rollout restart daemonset/kube-proxy -n kube-system
```

## 3. ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚µãƒ¼ãƒ“ã‚¹å•é¡Œ

### 3.1 ArgoCDå•é¡Œ

#### ç—‡çŠ¶: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åŒæœŸå¤±æ•—
```bash
kubectl get applications -n argocd
# STATUS: OutOfSync, Degraded
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. è©³ç´°ç¢ºèª
kubectl describe application <app-name> -n argocd

# 2. æ‰‹å‹•åŒæœŸ
kubectl patch application <app-name> -n argocd \
  --type merge -p '{"operation":{"sync":{"syncStrategy":{"hook":{}}}}}'

# 3. ãƒªãƒã‚¸ãƒˆãƒªæ¥ç¶šç¢ºèª
kubectl get secret -n argocd repo-<id> -o yaml

# 4. ArgoCDå†èµ·å‹•
kubectl rollout restart deployment -n argocd
```

#### ç—‡çŠ¶: ArgoCD UIã‚¢ã‚¯ã‚»ã‚¹ä¸å¯
```bash
# 503 Service Unavailable
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. PodçŠ¶æ…‹ç¢ºèª
kubectl get pods -n argocd
kubectl logs -n argocd deployment/argocd-server

# 2. Serviceç¢ºèª
kubectl get svc -n argocd
kubectl get ingress -n argocd

# 3. è¨¼æ˜æ›¸ç¢ºèª
kubectl get certificate -n argocd
kubectl describe certificate argocd-server-tls -n argocd
```

### 3.2 Harborå•é¡Œ

#### ç—‡çŠ¶: ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ—ãƒƒã‚·ãƒ¥å¤±æ•—
```bash
# Error: unauthorized: unauthorized to access repository
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. èªè¨¼æƒ…å ±ç¢ºèª
kubectl get secret harbor-auth -n arc-systems -o yaml

# 2. Harbor Podç¢ºèª
kubectl get pods -n harbor
kubectl logs -n harbor deployment/harbor-core

# 3. PVCç¢ºèª
kubectl get pvc -n harbor
kubectl describe pvc -n harbor

# 4. è¨¼æ˜æ›¸ç¢ºèª
kubectl get secret harbor-tls -n harbor
openssl s_client -connect 192.168.122.100:443 -servername harbor.local
```

#### ç—‡çŠ¶: Harborèµ·å‹•å¤±æ•—ï¼ˆPVC Pendingï¼‰
```bash
kubectl get pvc -n harbor
# STATUS: Pending
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. StorageClassç¢ºèª
kubectl get storageclass
kubectl describe storageclass local-path

# 2. PVCå†ä½œæˆ
kubectl delete pvc <pvc-name> -n harbor
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: <pvc-name>
  namespace: harbor
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: local-path
EOF
```

### 3.3 MetalLBå•é¡Œ

#### ç—‡çŠ¶: LoadBalancer IPå‰²ã‚Šå½“ã¦ã‚‰ã‚Œãªã„
```bash
kubectl get svc -n ingress-nginx
# EXTERNAL-IP: <pending>
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. MetalLB Podç¢ºèª
kubectl get pods -n metallb-system
kubectl logs -n metallb-system -l component=controller

# 2. IPã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ—ãƒ¼ãƒ«ç¢ºèª
kubectl get ipaddresspool -n metallb-system
kubectl describe ipaddresspool default-pool -n metallb-system

# 3. L2Advertisementç¢ºèª
kubectl get l2advertisement -n metallb-system

# 4. ARPç¢ºèª
arping 192.168.122.100

# 5. MetalLBå†ãƒ‡ãƒ—ãƒ­ã‚¤
kubectl delete namespace metallb-system
kubectl apply -f manifests/resources/infrastructure/metallb/
```

## 4. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å•é¡Œ

### 4.1 Podèµ·å‹•å•é¡Œ

#### ç—‡çŠ¶: ImagePullBackOff
```bash
kubectl get pods
# STATUS: ImagePullBackOff
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. ã‚¤ãƒ™ãƒ³ãƒˆç¢ºèª
kubectl describe pod <pod-name>

# 2. ã‚¤ãƒ¡ãƒ¼ã‚¸åç¢ºèª
kubectl get pod <pod-name> -o jsonpath='{.spec.containers[*].image}'

# 3. Secretç¢ºèª
kubectl get secret <pull-secret> -o yaml

# 4. æ‰‹å‹•ãƒ—ãƒ«ç¢ºèª
ssh k8suser@192.168.122.11
sudo crictl pull <image>

# 5. ãƒ¬ã‚¸ã‚¹ãƒˆãƒªæ¥ç¶šç¢ºèª
curl -v http://192.168.122.100/v2/
```

#### ç—‡çŠ¶: CrashLoopBackOff
```bash
kubectl get pods
# STATUS: CrashLoopBackOff
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. ãƒ­ã‚°ç¢ºèª
kubectl logs <pod-name>
kubectl logs <pod-name> --previous

# 2. ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ç¢ºèª
kubectl describe pod <pod-name> | grep -A 5 Limits

# 3. ç’°å¢ƒå¤‰æ•°ç¢ºèª
kubectl exec <pod-name> -- env

# 4. ãƒ‡ãƒãƒƒã‚°ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•
kubectl debug <pod-name> -it --image=busybox
```

### 4.2 ãƒªã‚½ãƒ¼ã‚¹å•é¡Œ

#### ç—‡çŠ¶: OOMKilled
```bash
kubectl describe pod <pod-name>
# Last State: Terminated
# Reason: OOMKilled
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ç¢ºèª
kubectl top pod <pod-name>
kubectl top nodes

# 2. ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™èª¿æ•´
kubectl edit deployment <deployment-name>
# resources:
#   limits:
#     memory: "1Gi"  # å¢—ã‚„ã™
#   requests:
#     memory: "512Mi"

# 3. HPAè¨­å®š
kubectl autoscale deployment <name> --min=2 --max=5 --cpu-percent=80
```

## 5. GitHub Actions Runnerå•é¡Œ

### 5.1 Runnerèµ·å‹•å•é¡Œ

#### ç—‡çŠ¶: Runner Queued/Offline
```bash
kubectl get ephemeralrunner -n arc-systems
# STATUS: Failed
```

**è§£æ±ºæ–¹æ³•:**
```bash
# 1. ServiceAccountç¢ºèª
kubectl get sa -n arc-systems
kubectl create sa github-actions-runner -n arc-systems

# 2. RBACç¢ºèª
kubectl get rolebinding -n arc-systems

# 3. Secretç¢ºèª
kubectl get secret -n arc-systems | grep github

# 4. Runnerå†ä½œæˆ
kubectl delete ephemeralrunner -n arc-systems --all
kubectl rollout restart deployment -n arc-systems

# 5. ãƒ­ã‚°ç¢ºèª
kubectl logs -n arc-systems -l app.kubernetes.io/name=gha-runner-scale-set
```

## ç·Šæ€¥å¯¾å¿œæ‰‹é †

### ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“åœæ­¢æ™‚
```bash
# 1. VMçŠ¶æ…‹ç¢ºèª
sudo virsh list --all

# 2. å¼·åˆ¶èµ·å‹•
for vm in k8s-control-plane-1 k8s-worker-1 k8s-worker-2; do
  sudo virsh start $vm
done

# 3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å¾©æ—§ç¢ºèª
kubectl get nodes
kubectl get pods --all-namespaces
```

### ãƒ‡ãƒ¼ã‚¿å¾©æ—§
```bash
# 1. etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©æ—§
ETCDCTL_API=3 etcdctl snapshot restore backup.db \
  --data-dir=/var/lib/etcd-restore

# 2. PV ãƒ‡ãƒ¼ã‚¿å¾©æ—§
kubectl get pv
# æ‰‹å‹•ã§ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼
```

## äºˆé˜²ä¿å®ˆ

### å®šæœŸç¢ºèªé …ç›®
```bash
# æ—¥æ¬¡
kubectl get nodes
kubectl get pods --all-namespaces | grep -v Running
kubectl top nodes

# é€±æ¬¡
df -h
sudo virsh list --all
kubectl get certificates --all-namespaces

# æœˆæ¬¡
kubectl get events --all-namespaces
sudo apt update && sudo apt upgrade
```

### ç›£è¦–è¨­å®š
```yaml
# Prometheus AlertRuleä¾‹
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: k8s-alerts
spec:
  groups:
  - name: kubernetes
    rules:
    - alert: NodeNotReady
      expr: up{job="kubernetes-nodes"} == 0
      for: 5m
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
```

---
*æœ€çµ‚æ›´æ–°: 2025-01-09*